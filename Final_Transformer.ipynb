{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9424515,"sourceType":"datasetVersion","datasetId":5654788}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Embedding, MultiHeadAttention, LayerNormalization, Dense, Dropout\nfrom tensorflow.keras.models import Model\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split","metadata":{"id":"XlPDPCXKYtBE","execution":{"iopub.status.busy":"2024-10-15T07:20:11.914960Z","iopub.execute_input":"2024-10-15T07:20:11.915782Z","iopub.status.idle":"2024-10-15T07:20:24.841529Z","shell.execute_reply.started":"2024-10-15T07:20:11.915745Z","shell.execute_reply":"2024-10-15T07:20:24.840688Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Load dataset\ndf = pd.read_csv('/kaggle/input/pali-to-english/output_file.csv')\n\n# Separate the input and target text\ninput_texts = df['input_text'].values\ntarget_texts = df['target_text'].values\n\n# Tokenization and padding (you can adjust max_length based on your data)\ntokenizer_pali = tf.keras.preprocessing.text.Tokenizer()\ntokenizer_english = tf.keras.preprocessing.text.Tokenizer()\n\ntokenizer_pali.fit_on_texts(input_texts)\ntokenizer_english.fit_on_texts(target_texts)\n\ninput_sequences = tokenizer_pali.texts_to_sequences(input_texts)\ntarget_sequences = tokenizer_english.texts_to_sequences(target_texts)\n\nmax_len_input = max([len(seq) for seq in input_sequences])\nmax_len_target = max([len(seq) for seq in target_sequences])\n\ninput_sequences = tf.keras.preprocessing.sequence.pad_sequences(input_sequences, maxlen=max_len_input, padding='post')\ntarget_sequences = tf.keras.preprocessing.sequence.pad_sequences(target_sequences, maxlen=max_len_target, padding='post')\n\n# Vocabulary sizes\nvocab_size_pali = len(tokenizer_pali.word_index) + 1\nvocab_size_english = len(tokenizer_english.word_index) + 1\n\n# Split data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(input_sequences, target_sequences[:, 1:], test_size=0.2, random_state=42)\ntarget_sequences_input = target_sequences[:, :-1]\n","metadata":{"id":"RnszVd5Yqrho","execution":{"iopub.status.busy":"2024-10-15T07:20:24.843361Z","iopub.execute_input":"2024-10-15T07:20:24.844080Z","iopub.status.idle":"2024-10-15T07:20:42.890249Z","shell.execute_reply.started":"2024-10-15T07:20:24.844032Z","shell.execute_reply":"2024-10-15T07:20:42.889434Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class Transformer(Model):\n    def __init__(self, vocab_size_input, vocab_size_target, embed_dim, num_heads, ff_dim, num_layers, dropout_rate=0.1):\n        super(Transformer, self).__init__()\n        self.embedding_input = Embedding(vocab_size_input, embed_dim)\n        self.embedding_target = Embedding(vocab_size_target, embed_dim)\n\n        self.encoder_layers = [\n            [MultiHeadAttention(num_heads, embed_dim // num_heads),\n             LayerNormalization(epsilon=1e-6),\n             Dense(ff_dim, activation='relu'),\n             Dense(embed_dim),\n             Dropout(dropout_rate)]\n            for _ in range(num_layers)\n        ]\n\n        self.decoder_layers = [\n            [MultiHeadAttention(num_heads, embed_dim // num_heads),\n             MultiHeadAttention(num_heads, embed_dim // num_heads),\n             LayerNormalization(epsilon=1e-6),\n             Dense(ff_dim, activation='relu'),\n             Dense(embed_dim),\n             Dropout(dropout_rate)]\n            for _ in range(num_layers)\n        ]\n\n        self.final_layer = Dense(vocab_size_target)\n\n    def call(self, inputs, training=False):\n        input_seq, target_seq = inputs\n        input_embedding = self.embedding_input(input_seq)\n        target_embedding = self.embedding_target(target_seq)\n\n        x = input_embedding\n        for mha, ln, ff1, ff2, drop in self.encoder_layers:\n            attn_output = mha(x, x)\n            x = ln(x + attn_output)\n            ffn_output = ff2(ff1(x))\n            x = drop(x + ffn_output, training=training)\n\n        y = target_embedding\n        for mha1, mha2, ln, ff1, ff2, drop in self.decoder_layers:\n            attn_output1 = mha1(y, y)\n            attn_output2 = mha2(attn_output1, x)\n            y = ln(y + attn_output2)\n            ffn_output = ff2(ff1(y))\n            y = drop(y + ffn_output, training=training)\n\n        return self.final_layer(y)\n","metadata":{"id":"KVXQmxKTq3My","execution":{"iopub.status.busy":"2024-10-15T07:20:42.891500Z","iopub.execute_input":"2024-10-15T07:20:42.891806Z","iopub.status.idle":"2024-10-15T07:20:42.903350Z","shell.execute_reply.started":"2024-10-15T07:20:42.891774Z","shell.execute_reply":"2024-10-15T07:20:42.902457Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Model parameters\nembed_dim = 256  # Embedding size\nnum_heads = 8    # Number of attention heads\nff_dim = 512     # Feed-forward network size\nnum_layers = 4   # Number of encoder/decoder layers\n\ntransformer = Transformer(vocab_size_pali, vocab_size_english, embed_dim, num_heads, ff_dim, num_layers)\n\n# Compile the model with accuracy metric\ntransformer.compile(optimizer='adam',\n                    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n                    metrics=['accuracy'])\n\n# Train the model for 10 epochs with a batch size of 124\ntransformer.fit([X_train, target_sequences_input[:len(X_train)]],\n                y_train,\n                batch_size=16,  # or 32\n                epochs=10)\n\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_oY_L4mRq4DB","outputId":"b2d3adec-3394-44ea-c3db-9cd1c13cc1ef","execution":{"iopub.status.busy":"2024-10-15T07:20:42.905393Z","iopub.execute_input":"2024-10-15T07:20:42.905678Z","iopub.status.idle":"2024-10-15T18:33:29.592199Z","shell.execute_reply.started":"2024-10-15T07:20:42.905648Z","shell.execute_reply":"2024-10-15T18:33:29.591264Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Epoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1728976866.833967      82 service.cc:145] XLA service 0x793ed4003e70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1728976866.834020      82 service.cc:153]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1728976866.834024      82 service.cc:153]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nW0000 00:00:1728976868.619541      82 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1728976887.367542     110 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_70', 1076 bytes spill stores, 1076 bytes spill loads\n\nI0000 00:00:1728976891.008395     108 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_457', 332 bytes spill stores, 312 bytes spill loads\n\nI0000 00:00:1728976891.453097     109 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_457', 4 bytes spill stores, 4 bytes spill loads\n\nI0000 00:00:1728976895.559436     107 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_440', 100 bytes spill stores, 100 bytes spill loads\n\nI0000 00:00:1728976897.781705     110 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_442', 1764 bytes spill stores, 1764 bytes spill loads\n\nI0000 00:00:1728976899.328775     109 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_459', 12 bytes spill stores, 12 bytes spill loads\n\nI0000 00:00:1728976902.150301     108 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_427', 100 bytes spill stores, 100 bytes spill loads\n\nI0000 00:00:1728976915.368777     110 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_467', 12 bytes spill stores, 12 bytes spill loads\n\nI0000 00:00:1728976916.476886     108 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_444', 1764 bytes spill stores, 1764 bytes spill loads\n\nI0000 00:00:1728976943.224366      82 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m6607/6608\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 618ms/step - accuracy: 0.9753 - loss: 0.3292","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1728981036.880356     224 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_475', 12 bytes spill stores, 12 bytes spill loads\n\nI0000 00:00:1728981043.701647     224 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_467', 1764 bytes spill stores, 1764 bytes spill loads\n\nI0000 00:00:1728981046.704396     222 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_452', 1764 bytes spill stores, 1764 bytes spill loads\n\nI0000 00:00:1728981058.585968     222 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_446', 100 bytes spill stores, 100 bytes spill loads\n\nI0000 00:00:1728981063.911056     224 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_458', 1764 bytes spill stores, 1764 bytes spill loads\n\nI0000 00:00:1728981066.536016     225 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_448', 100 bytes spill stores, 100 bytes spill loads\n\nI0000 00:00:1728981067.200686     225 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_473', 4 bytes spill stores, 4 bytes spill loads\n\nI0000 00:00:1728981068.830381     222 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_427', 100 bytes spill stores, 100 bytes spill loads\n\nI0000 00:00:1728981069.364646     225 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_473', 332 bytes spill stores, 312 bytes spill loads\n\nI0000 00:00:1728981074.531045     224 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_483', 12 bytes spill stores, 12 bytes spill loads\n\nI0000 00:00:1728981086.030665     223 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_70', 1076 bytes spill stores, 1076 bytes spill loads\n\nI0000 00:00:1728981086.670983     222 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_440', 100 bytes spill stores, 100 bytes spill loads\n\nI0000 00:00:1728981087.488076     225 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_450', 1764 bytes spill stores, 1764 bytes spill loads\n\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m6608/6608\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4265s\u001b[0m 631ms/step - accuracy: 0.9753 - loss: 0.3292\nEpoch 2/10\n\u001b[1m6608/6608\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4033s\u001b[0m 610ms/step - accuracy: 0.9766 - loss: 0.2739\nEpoch 3/10\n\u001b[1m6608/6608\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4015s\u001b[0m 608ms/step - accuracy: 0.9767 - loss: 0.2723\nEpoch 4/10\n\u001b[1m6608/6608\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4012s\u001b[0m 607ms/step - accuracy: 0.9767 - loss: 0.2719\nEpoch 5/10\n\u001b[1m6608/6608\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4012s\u001b[0m 607ms/step - accuracy: 0.9767 - loss: 0.2719\nEpoch 6/10\n\u001b[1m6608/6608\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4011s\u001b[0m 607ms/step - accuracy: 0.9767 - loss: 0.2713\nEpoch 7/10\n\u001b[1m6608/6608\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4010s\u001b[0m 607ms/step - accuracy: 0.9767 - loss: 0.2718\nEpoch 8/10\n\u001b[1m6608/6608\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4009s\u001b[0m 607ms/step - accuracy: 0.9766 - loss: 0.2717\nEpoch 9/10\n\u001b[1m6608/6608\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4002s\u001b[0m 606ms/step - accuracy: 0.9767 - loss: 0.2714\nEpoch 10/10\n\u001b[1m6608/6608\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3996s\u001b[0m 605ms/step - accuracy: 0.9767 - loss: 0.2709\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x793fbefc5f60>"},"metadata":{}}]},{"cell_type":"code","source":"# Evaluate on the validation set\nval_loss, val_accuracy = transformer.evaluate([X_val, target_sequences_input[len(X_train):]], y_val)\n\nprint(f\"Validation Loss: {val_loss}\")\nprint(f\"Validation Accuracy: {val_accuracy}\")\n","metadata":{"id":"cwsHSdEjrABw","execution":{"iopub.status.busy":"2024-10-15T18:33:29.593662Z","iopub.execute_input":"2024-10-15T18:33:29.594035Z","iopub.status.idle":"2024-10-15T18:39:33.944114Z","shell.execute_reply.started":"2024-10-15T18:33:29.593993Z","shell.execute_reply":"2024-10-15T18:39:33.943279Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"W0000 00:00:1729017211.134732      81 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\nI0000 00:00:1729017213.648433     447 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_80', 12 bytes spill stores, 12 bytes spill loads\n\nI0000 00:00:1729017214.245391     448 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_78', 332 bytes spill stores, 312 bytes spill loads\n\nI0000 00:00:1729017214.722991     445 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_78', 4 bytes spill stores, 4 bytes spill loads\n\nI0000 00:00:1729017217.725884     446 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_88', 12 bytes spill stores, 12 bytes spill loads\n\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m825/826\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 396ms/step - accuracy: 0.9768 - loss: 0.2701","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1729017556.263868      79 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\nI0000 00:00:1729017557.064799     476 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_78', 4 bytes spill stores, 4 bytes spill loads\n\nI0000 00:00:1729017559.293940     477 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_78', 332 bytes spill stores, 312 bytes spill loads\n\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m826/826\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m364s\u001b[0m 417ms/step - accuracy: 0.9768 - loss: 0.2701\nValidation Loss: 0.27197015285491943\nValidation Accuracy: 0.9765858054161072\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"id":"e54vAOEDr-aU"},"execution_count":null,"outputs":[]}]}